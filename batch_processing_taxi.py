from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, count, avg, sum, min, max, stddev, 
    when, isnan, isnull, hour, dayofweek, 
    datediff, to_timestamp, round as spark_round
)
from pyspark.sql.types import DoubleType
import time

print("=" * 80)
print("ğŸš• NYC TAXI - BATCH PROCESSING & EDA")
print("=" * 80)

# ============================================
# 1. CREAR SESIÃ“N DE SPARK
# ============================================
print("\nğŸ“Š Iniciando Spark Session...")
spark = SparkSession.builder \
    .appName("NYC_Taxi_Batch_Processing") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")
print("âœ… Spark Session iniciado correctamente\n")

# ============================================
# 2. CARGAR DATOS
# ============================================
print("ğŸ“¥ PASO 1: CARGA DE DATOS")
print("-" * 80)

# Cambiar la ruta segÃºn tu archivo
# OpciÃ³n A: CSV generado
csv_path = "nyc_taxi_data.csv"

print(f"Leyendo archivo: {csv_path}")
start_time = time.time()

# Leer CSV
df = spark.read.csv(csv_path, header=True, inferSchema=True)


load_time = time.time() - start_time
print(f"âœ… Datos cargados en {load_time:.2f} segundos")
print(f"ğŸ“Š Total de registros: {df.count()}")
print(f"ğŸ“‹ Total de columnas: {len(df.columns)}")

# ============================================
# 3. EXPLORACIÃ“N INICIAL
# ============================================
print("\nğŸ“Š PASO 2: ANÃLISIS EXPLORATORIO DE DATOS (EDA)")
print("-" * 80)

print("\nğŸ” Esquema del DataFrame:")
df.printSchema()

print("\nğŸ“‹ Primeros 10 registros:")
df.show(10, truncate=False)

print("\nğŸ“Š EstadÃ­sticas Descriptivas:")
df.describe().show()

# ============================================
# 4. CALIDAD DE DATOS
# ============================================
print("\nğŸ§¹ PASO 3: ANÃLISIS DE CALIDAD DE DATOS")
print("-" * 80)

# Contar valores nulos por columna
print("\nâŒ Valores nulos por columna:")
total_records = df.count()
for column in df.columns:
    # Solo usar isnan() para columnas numÃ©ricas
    col_type = df.schema[column].dataType.typeName()
    if col_type in ['double', 'float']:
        null_count = df.filter(col(column).isNull() | isnan(col(column))).count()
    else:
        null_count = df.filter(col(column).isNull()).count()
    
    if null_count > 0:
        percentage = (null_count / total_records) * 100
        print(f"   {column}: {null_count} ({percentage:.2f}%)")
      
# Detectar valores anÃ³malos
print("\nâš ï¸  Valores AnÃ³malos Detectados:")

# Pasajeros = 0
zero_passengers = df.filter(col("passenger_count") == 0).count()
print(f"   Viajes con 0 pasajeros: {zero_passengers}")

# Distancia = 0
zero_distance = df.filter(col("trip_distance") == 0).count()
print(f"   Viajes con distancia 0: {zero_distance}")

# Tarifas negativas
negative_fare = df.filter(col("fare_amount") < 0).count()
print(f"   Tarifas negativas: {negative_fare}")

# Tarifas muy altas (posibles outliers)
high_fare = df.filter(col("fare_amount") > 200).count()
print(f"   Tarifas > $200: {high_fare}")

# ============================================
# 5. LIMPIEZA DE DATOS
# ============================================
print("\nğŸ§¹ PASO 4: LIMPIEZA Y TRANSFORMACIÃ“N DE DATOS")
print("-" * 80)

print("\nğŸ”§ Aplicando reglas de limpieza...")

# Crear DataFrame limpio
df_clean = df.filter(
    (col("passenger_count") > 0) &
    (col("trip_distance") > 0) &
    (col("fare_amount") > 0) &
    (col("fare_amount") < 500) &  # Eliminar outliers extremos
    (col("total_amount") > 0)
)

# Eliminar duplicados
df_clean = df_clean.dropDuplicates()

records_removed = df.count() - df_clean.count()
print(f"âœ… Registros eliminados: {records_removed}")
print(f"âœ… Registros limpios: {df_clean.count()}")

# Crear nuevas columnas derivadas
print("\nğŸ”§ Creando columnas derivadas...")

df_clean = df_clean.withColumn(
    "trip_duration_minutes",
    (col("dropoff_datetime").cast("long") - col("pickup_datetime").cast("long")) / 60
)

df_clean = df_clean.withColumn(
    "fare_per_mile",
    spark_round(col("fare_amount") / col("trip_distance"), 2)
)

df_clean = df_clean.withColumn(
    "tip_percentage",
    spark_round((col("tip_amount") / col("fare_amount")) * 100, 2)
)

print("âœ… Columnas derivadas creadas")

# ============================================
# 6. ANÃLISIS EXPLORATORIO AVANZADO
# ============================================
print("\nğŸ“Š PASO 5: ANÃLISIS EXPLORATORIO AVANZADO")
print("-" * 80)

# AnÃ¡lisis por zona de recogida
print("\nğŸ—ºï¸  AnÃ¡lisis por Zona de Recogida:")
zone_analysis = df_clean.groupBy("pickup_zone").agg(
    count("trip_id").alias("total_trips"),
    spark_round(avg("fare_amount"), 2).alias("avg_fare"),
    spark_round(avg("trip_distance"), 2).alias("avg_distance"),
    spark_round(avg("tip_amount"), 2).alias("avg_tip"),
    spark_round(sum("total_amount"), 2).alias("total_revenue")
).orderBy(col("total_trips").desc())

zone_analysis.show(10)

# AnÃ¡lisis por tipo de pago
print("\nğŸ’³ AnÃ¡lisis por Tipo de Pago:")
payment_analysis = df_clean.groupBy("payment_type").agg(
    count("trip_id").alias("total_trips"),
    spark_round(avg("fare_amount"), 2).alias("avg_fare"),
    spark_round(avg("tip_amount"), 2).alias("avg_tip"),
    spark_round(sum("total_amount"), 2).alias("total_revenue")
).orderBy(col("total_trips").desc())

payment_analysis.show()

# AnÃ¡lisis por nÃºmero de pasajeros
print("\nğŸ‘¥ AnÃ¡lisis por NÃºmero de Pasajeros:")
passenger_analysis = df_clean.groupBy("passenger_count").agg(
    count("trip_id").alias("total_trips"),
    spark_round(avg("fare_amount"), 2).alias("avg_fare"),
    spark_round(avg("trip_distance"), 2).alias("avg_distance")
).orderBy("passenger_count")

passenger_analysis.show()

# Rutas mÃ¡s populares
print("\nğŸ›£ï¸  Top 10 Rutas MÃ¡s Populares:")
routes = df_clean.groupBy("pickup_zone", "dropoff_zone").agg(
    count("trip_id").alias("total_trips"),
    spark_round(avg("fare_amount"), 2).alias("avg_fare")
).orderBy(col("total_trips").desc())

routes.show(10)

# ============================================
# 7. ESTADÃSTICAS AVANZADAS
# ============================================
print("\nğŸ“ˆ PASO 6: ESTADÃSTICAS AVANZADAS")
print("-" * 80)

print("\nğŸ“Š MÃ©tricas Generales:")
general_stats = df_clean.agg(
    count("trip_id").alias("Total Trips"),
    spark_round(avg("fare_amount"), 2).alias("Avg Fare"),
    spark_round(min("fare_amount"), 2).alias("Min Fare"),
    spark_round(max("fare_amount"), 2).alias("Max Fare"),
    spark_round(stddev("fare_amount"), 2).alias("Std Dev Fare"),
    spark_round(avg("trip_distance"), 2).alias("Avg Distance"),
    spark_round(sum("total_amount"), 2).alias("Total Revenue")
)

general_stats.show(vertical=True)

# DistribuciÃ³n de tarifas
print("\nğŸ’° DistribuciÃ³n de Tarifas:")
fare_distribution = df_clean.selectExpr(
    "count(*) as total",
    "sum(case when fare_amount < 10 then 1 else 0 end) as under_10",
    "sum(case when fare_amount >= 10 and fare_amount < 20 then 1 else 0 end) as fare_10_20",
    "sum(case when fare_amount >= 20 and fare_amount < 30 then 1 else 0 end) as fare_20_30",
    "sum(case when fare_amount >= 30 and fare_amount < 50 then 1 else 0 end) as fare_30_50",
    "sum(case when fare_amount >= 50 then 1 else 0 end) as over_50"
)

fare_distribution.show()

# ============================================
# 8. GUARDAR RESULTADOS
# ============================================
print("\nğŸ’¾ PASO 7: ALMACENAMIENTO DE RESULTADOS")
print("-" * 80)

# Crear directorio para resultados
output_dir = "output_batch_processing"

print(f"\nğŸ“ Guardando resultados en: {output_dir}/")

# Guardar DataFrame limpio
print("   1. Guardando datos limpios (Parquet)...")
df_clean.write.mode("overwrite").parquet(f"{output_dir}/clean_data")
print("   âœ… Datos limpios guardados")

# Guardar anÃ¡lisis por zona
print("   2. Guardando anÃ¡lisis por zona (CSV)...")
zone_analysis.write.mode("overwrite").csv(f"{output_dir}/zone_analysis", header=True)
print("   âœ… AnÃ¡lisis por zona guardado")

# Guardar anÃ¡lisis de pagos
print("   3. Guardando anÃ¡lisis de pagos (CSV)...")
payment_analysis.write.mode("overwrite").csv(f"{output_dir}/payment_analysis", header=True)
print("   âœ… AnÃ¡lisis de pagos guardado")

# Guardar rutas populares
print("   4. Guardando rutas populares (CSV)...")
routes.limit(100).write.mode("overwrite").csv(f"{output_dir}/popular_routes", header=True)
print("   âœ… Rutas populares guardadas")

# ============================================
# 9. RESUMEN FINAL
# ============================================
print("\n" + "=" * 80)
print("âœ… PROCESAMIENTO BATCH COMPLETADO EXITOSAMENTE")
print("=" * 80)

print(f"""
ğŸ“Š RESUMEN DE RESULTADOS:
   â€¢ Registros originales: {df.count()}
   â€¢ Registros limpios: {df_clean.count()}
   â€¢ Registros eliminados: {records_removed}
   â€¢ Tiempo de procesamiento: {time.time() - start_time:.2f} segundos
   
ğŸ“ ARCHIVOS GENERADOS:
   â€¢ {output_dir}/clean_data/ (Parquet)
   â€¢ {output_dir}/zone_analysis/ (CSV)
   â€¢ {output_dir}/payment_analysis/ (CSV)
   â€¢ {output_dir}/popular_routes/ (CSV)
   
ğŸ¯ SIGUIENTE PASO:
   Revisar los archivos generados en la carpeta: {output_dir}/
""")

# Detener Spark
spark.stop()
print("ğŸ›‘ Spark Session detenido\n")
